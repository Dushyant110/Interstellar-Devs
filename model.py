# -*- coding: utf-8 -*-
"""PSA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IBTnfnEGixSfLyZddtV97zKrYsp4kjXv
"""

import numpy as np
from scipy.signal import find_peaks, peak_prominences, peak_widths
from scipy.ndimage import gaussian_filter
import matplotlib.pyplot as plt
from astropy.io import fits
import os
import pandas as pd

import os

# Directory containing the .lc files
lc_directory = '/content/'  # Replace with your actual directory
# Get a list of all .lc files in the directory
lc_files = [f for f in os.listdir(lc_directory) if f.endswith('.lc')]
# Write the full paths to lc_file_paths.txt
with open('lc_file_paths.txt', 'w') as file:
    for lc_file in lc_files:
        file.write(os.path.join(lc_directory, lc_file) + '\n')

print("File paths written to lc_file_paths.txt")

#extracting useful features
def evaluate(filename):
    hdulist = fits.open(filename)
    times, rates = hdulist[1].data["TIME"], hdulist[1].data["RATE"]
    hdulist.close()

    found = []
    g = gaussian_filter(rates, sigma=10)
    peaks, _ = find_peaks(g)

    prominences, _, _ = peak_prominences(g, peaks)
    if len(prominences) == 0:
        return []

    selected = prominences > 0.5 * (np.min(prominences) + np.max(prominences))
    if len(selected) == 0:
        return []

    top = peaks[selected]
    if len(top) > 10:
        return []
    eigth_peak_widths = peak_widths(g, top, rel_height=0.8)

    per = np.percentile(g, 75)

    for i in range(len(g[top])):
        found.append({
            'start_time': times[int(eigth_peak_widths[2][i])],
            'end_time': times[int(eigth_peak_widths[3][i])],
            'max_time': times[top[i]],
            'peak_width': eigth_peak_widths[0][i],
            'peak_risetime': top[i] - eigth_peak_widths[2][i],
            'peak_falltime': eigth_peak_widths[3][i] - top[i],
            'peak_flux': rates[top[i]],
        })

    return found

# First we downloaded all the .lc files and created a text file containing the paths.
# Now this code runs through every file and creates a dataframe with the above features
def process(file_paths_file):
    with open(file_paths_file, 'r') as file:
        file_paths = file.read().splitlines()

    # Initialize an empty list to store data frames
    dataframes = []

    for file_path in file_paths:
        found = evaluate(file_path)
        # Convert found list of dicts to a DataFrame and add to list
        if found:  # If found is not empty
            df_temp = pd.DataFrame(found)
            dataframes.append(df_temp)

    # Concatenate all dataframes in the list into a single DataFrame
    if dataframes:  # If there is any data to concatenate
        df = pd.concat(dataframes, ignore_index=True)
        df.to_csv('feature_data.csv', index=False)
    else:
        print("No data to save to CSV.")

file_paths_file = 'lc_file_paths.txt'

process(file_paths_file)

df_data=pd.read_csv('feature_data.csv')

df_data.head()

df_data.shape

import astropy
import scipy
from astropy.io import fits
import xml.etree.ElementTree as ET
import matplotlib.pyplot as plt
from astropy.table import Table
import pandas as pd
import numpy as np
from scipy.ndimage import gaussian_filter1d
from scipy.signal import find_peaks, peak_prominences

import numpy as np
import pandas as pd
from scipy.ndimage import gaussian_filter1d
from scipy.signal import find_peaks, peak_prominences, peak_widths
import astropy.io.fits as fits
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

class x_ray_burst:
    def __init__(self, gti_filename, lc_filename):
        self.gti_filename = gti_filename
        self.lc_filename = lc_filename

    def data(self):
        # Load GTI and LC data
        gti_hdulist = fits.open(self.gti_filename)
        gti_data = gti_hdulist[1].data
        gti_start_times = gti_data['START']
        gti_end_times = gti_data['STOP']
        gti_hdulist.close()

        lc_hdulist = fits.open(self.lc_filename)
        lc_data = lc_hdulist[1].data
        lc_times = lc_data['TIME']
        lc_rates = lc_data['RATE']
        lc_hdulist.close()

        return gti_start_times, gti_end_times, lc_times, lc_rates

    def combined_data(self):
        gti_start_times, gti_end_times, lc_times, lc_rates = self.data()
        filtered_lc_times = []
        filtered_lc_values = []

        # Filter the LC data based on valid GTI times
        for i in range(len(lc_times)):
            for j in range(len(gti_start_times)):
                if gti_start_times[j] <= lc_times[i] <= gti_end_times[j]:
                    filtered_lc_times.append(lc_times[i])
                    filtered_lc_values.append(lc_rates[i])
                    break

        return filtered_lc_times, filtered_lc_values

    def plot_combined_data(self):
        times, values = self.combined_data()
        plt.plot(times, values)
        plt.xlabel("Time")
        plt.ylabel("XRay Flux")
        plt.show()

    def peaks(self):
        times, values = self.combined_data()
        smoothed_values = gaussian_filter1d(values, sigma=5)

        # Plot the smoothed data
        plt.figure(figsize=(12, 6))
        plt.plot(times, smoothed_values, label='Smoothed Data (Gaussian)')
        plt.xlabel("Time")
        plt.ylabel("XRay Flux")
        plt.legend()
        plt.grid(True)
        plt.show()

        # Detect peaks
        peaks, _ = find_peaks(smoothed_values, height=10, prominence=1000)
        prominences = peak_prominences(smoothed_values, peaks)[0]

        # Plot detected peaks
        plt.figure(figsize=(10, 6))
        plt.plot(smoothed_values, label='Smoothed Data')
        plt.plot(peaks, smoothed_values[peaks], 'ro', label='Detected Peaks')
        plt.xlabel('Time')
        plt.ylabel('Value')
        plt.legend()
        plt.grid(True)
        plt.show()

        return times, smoothed_values, peaks

    def extract_features(self):
        times, smoothed_values, peaks = self.peaks()
        features = []

        for i in range(len(peaks)):
            peak_idx = peaks[i]
            start_idx = np.where(smoothed_values[:peak_idx] <= 1)[0]
            end_idx = np.where(smoothed_values[peak_idx:] <= 1)[0]

            if len(start_idx) > 0:
                burst_start = times[start_idx[-1]]
            else:
                burst_start = times[0]

            if len(end_idx) > 0:
                burst_end = times[peak_idx + end_idx[0]]
            else:
                burst_end = times[-1]

            burst_duration = burst_end - burst_start
            peak_flux = smoothed_values[peak_idx]
            rise_time = times[peak_idx] - burst_start
            decay_time = burst_end - times[peak_idx]

            features.append({
                'burst_start': burst_start,
                'burst_end': burst_end,
                'peak_time': times[peak_idx],
                'burst_duration': burst_duration,
                'peak_flux': peak_flux,
                'rise_time': rise_time,
                'decay_time': decay_time
            })

        return pd.DataFrame(features)

    def cluster_bursts(self, n_clusters=5):
        # Extract features for K-means clustering
        df_features = self.extract_features()

        # Select features for clustering
        X = df_features[['burst_duration', 'peak_flux', 'rise_time', 'decay_time']].values

        # Scale features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Apply K-means clustering
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        df_features['cluster'] = kmeans.fit_predict(X_scaled)

        # Map cluster labels to meaningful burst types
        cluster_labels = {
            0: 'Normal Burst',
            1: 'Miniburst',
            2: 'Mesoburst',
            3: 'Structured Burst',
            4: 'Microburst'
        }

        df_features['burst_type'] = df_features['cluster'].map(cluster_labels)

        # Print the results
        print(df_features[['burst_start', 'burst_end', 'peak_time', 'burst_duration', 'peak_flux', 'rise_time', 'decay_time', 'burst_type']])

        return df_features

    def visualize_clusters(self, df_features):
        # Scatter plot to visualize clusters (using two features for simplicity)
        plt.figure(figsize=(10, 6))
        plt.scatter(df_features['peak_flux'], df_features['burst_duration'], c=df_features['cluster'], cmap='viridis', s=100)
        plt.xlabel("Peak Flux")
        plt.ylabel("Burst Duration")
        plt.title("K-Means Clustering of X-ray Bursts")
        plt.colorbar(label='Cluster')
        plt.grid(True)
        plt.show()

# Assign the file paths directly instead of using input()
y = "/content/ch2_xsm_20240906_v1_level2.lc"
x = "/content/ch2_xsm_20240906_v1_level2.gti"

# No need to use strip() as there are no extra quotes
gti_filename = x
lc_filename = y

# Instantiate the x_ray_burst class and call its methods
burst = x_ray_burst(gti_filename, lc_filename)

# Plot the combined data
burst.plot_combined_data()

# Detect and visualize peaks
burst.peaks()

# Get combined data and calculate features
times, smoothed_values = burst.combined_data()
burst.extract_features() # Call the correct method name: extract_features()

# Apply clustering and classify bursts into 5 types
df_burst_classification = burst.cluster_bursts(n_clusters=5)





# Visualize the clusters
burst.visualize_clusters(df_burst_classification)

df_features = burst.extract_features()

# Display extracted features
print(df_features.head())

# Target Variables: we will predict 'rise_time', 'decay_time', and 'peak_flux'
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
X = df_features[['burst_duration', 'peak_flux', 'rise_time', 'decay_time']]
y_rise_time = df_features['rise_time']
y_decay_time = df_features['decay_time']
y_peak_flux = df_features['peak_flux']

# Split data into training and testing sets (e.g., for rise time prediction)
X_train, X_test, y_train_rise, y_test_rise = train_test_split(X, y_rise_time, test_size=0.2, random_state=42)
X_train, X_test, y_train_decay, y_test_decay = train_test_split(X, y_decay_time, test_size=0.2, random_state=42)
X_train, X_test, y_train_peak, y_test_peak = train_test_split(X, y_peak_flux, test_size=0.2, random_state=42)

# Initialize Random Forest Regressor
rf_rise_time = RandomForestRegressor(n_estimators=100, random_state=42)
rf_decay_time = RandomForestRegressor(n_estimators=100, random_state=42)
rf_peak_flux = RandomForestRegressor(n_estimators=100, random_state=42)

# Fit the model to the training data
rf_rise_time.fit(X_train, y_train_rise)

rf_decay_time.fit(X_train, y_train_decay)

rf_peak_flux.fit(X_train, y_train_peak)

# Predict on the test set
y_pred_rise = rf_rise_time.predict(X_test)
y_pred_decay = rf_decay_time.predict(X_test)
y_pred_peak = rf_peak_flux.predict(X_test)

# Evaluate the model's performance
from sklearn.metrics import r2_score, mean_squared_error
print("Rise Time Prediction")
print("R² Score:", r2_score(y_test_rise, y_pred_rise))
print("Mean Squared Error:", mean_squared_error(y_test_rise, y_pred_rise))

print("\nDecay Time Prediction")
print("R² Score:", r2_score(y_test_decay, y_pred_decay))
print("Mean Squared Error:", mean_squared_error(y_test_decay, y_pred_decay))

print("\nPeak Flux Prediction")
print("R² Score:", r2_score(y_test_peak, y_pred_peak))
print("Mean Squared Error:", mean_squared_error(y_test_peak, y_pred_peak))

# Feature Importance
print("\nFeature Importance for Rise Time Prediction:")
print(rf_rise_time.feature_importances_)

# Feature Importance
feature_importance = rf_rise_time.feature_importances_
feature_names = ['burst_duration', 'peak_flux', 'rise_time', 'decay_time']

# Visualize the feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_importance, color='skyblue')
plt.xlabel("Feature Importance")
plt.title("Feature Importance for Rise Time Prediction (Random Forest)")
plt.show()

# Feature Importance
feature_importance = rf_decay_time.feature_importances_
feature_names = ['burst_duration', 'peak_flux', 'rise_time', 'decay_time']

# Visualize the feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_importance, color='skyblue')
plt.xlabel("Feature Importance")
plt.title("Feature Importance for decay Time Prediction (Random Forest)")
plt.show()

# Feature Importance
feature_importance = rf_peak_flux.feature_importances_
feature_names = ['burst_duration', 'peak_flux', 'rise_time', 'decay_time']

# Visualize the feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_importance, color='skyblue')
plt.xlabel("Feature Importance")
plt.title("Feature Importance for Peak Flux Prediction (Random Forest)")
plt.show()


